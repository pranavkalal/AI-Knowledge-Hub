# configs/runtime.yaml
# Master runtime configuration for the Knowledge Hub pipeline.
# - Switch orchestrators between native and LangChain via 'orchestrator'.
# - Update adapters/models here instead of touching source code.
# - Retriever options (filters, multi-query, compression) live under 'retrieval'.
# - LangChain-specific toggles (tracing, streaming callbacks) live under 'langchain'.

orchestrator: langchain            # choose between "langchain" or "native"

embedder:
  adapter: bge_local
  model: BAAI/bge-small-en-v1.5
  normalize: true                  # recommended so inner-product â‰ˆ cosine

vector_store:
  adapter: faiss_local
  path: data/embeddings/vectors.faiss
  ids:  data/embeddings/ids.npy
  meta: data/staging/chunks.jsonl

reranker:
  adapter: bge_reranker            # or "none"
  model: BAAI/bge-reranker-base

llm:
  adapter: ollama                  # set to "openai" if you like invoices
  model: llama3.1:8b      # must match what you `ollama pull`
  temperature: 0.2
  max_output_tokens: 800

retrieval:
  k: 6
  mode: dense                      # dense | bm25 | hybrid (your store decides)
  rerank: false
  filters:
    year_min: 2019
    year_max: 2025
    # add other keys your store supports, e.g. contains, neighbors, per_doc

  use_multiquery: false
  use_compression: false

langchain:
  trace: true
  stream: true
