# configs/runtime.yaml
orchestrator: langchain            # choose between "langchain" or "native"

embedder:
  adapter: bge_local
  model: BAAI/bge-small-en-v1.5
  normalize: true                  # recommended so inner-product â‰ˆ cosine

vector_store:
  adapter: faiss_local
  path: data/embeddings/vectors.faiss
  ids:  data/embeddings/ids.npy
  meta: data/staging/chunks.jsonl

reranker:
  adapter: bge_reranker            # or "none"
  model: BAAI/bge-reranker-base

llm:
  adapter: ollama                  # set to "openai" if you like invoices
  model: llama3.1:8b      # must match what you `ollama pull`
  temperature: 0.2
  max_output_tokens: 800

retrieval:
  k: 6
  mode: dense                      # dense | bm25 | hybrid (your store decides)
  rerank: true
  filters:
    year_min: 2019
    year_max: 2025
    # add other keys your store supports, e.g. contains, neighbors, per_doc
